{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be315542-50e5-48ee-84f4-919fe048ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from pycocotools.coco import COCO\n",
    "# from pycocotools import mask as maskUtils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from segmentation_models_pytorch import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe73055-a2c8-4e4d-a1ba-d3fc950e40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fe5fb-065a-442a-b5e7-1c8efd9fc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(annotation_file, image_dir, mask_dir): \n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "    # Initialize COCO API\n",
    "    coco = COCO(annotation_file)\n",
    "\n",
    "# Get all image IDs\n",
    "    image_ids = coco.getImgIds()\n",
    "\n",
    "# Define category mapping\n",
    "    categories = coco.loadCats(coco.getCatIds())\n",
    "    category_mapping = {cat['id']: cat['name'] for cat in categories}\n",
    "    num_classes = len(category_mapping)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Generating Multi-Class Masks\"):\n",
    "    # Load image info\n",
    "        image_info = coco.loadImgs(image_id)[0]\n",
    "        height, width = image_info['height'], image_info['width']\n",
    "\n",
    "    # Initialize blank multi-class mask\n",
    "        multi_class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Get all annotations for the image\n",
    "        ann_ids = coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = coco.loadAnns(ann_ids)\n",
    "\n",
    "        for ann in annotations:\n",
    "            category_id = ann['category_id']\n",
    "            ann_id = ann['id']\n",
    "\n",
    "        # Generate mask for the annotation\n",
    "            if ann['iscrowd']:\n",
    "                rle = ann['segmentation']\n",
    "                mask = maskUtils.decode(rle)\n",
    "            else:\n",
    "                mask = coco.annToMask(ann)\n",
    "\n",
    "        # Assign category_id to mask pixels\n",
    "        # Handle overlapping by assigning the category_id (latest object overwrites)\n",
    "            multi_class_mask[mask > 0] = category_id\n",
    "\n",
    "    # Save the multi-class mask as PNG\n",
    "        image_filename = image_info['file_name']\n",
    "        mask_filename = os.path.splitext(image_filename)[0] + '_mask.png'\n",
    "        mask_path = os.path.join(mask_dir, mask_filename)\n",
    "        cv2.imwrite(mask_path, multi_class_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a5a8e-5c15-4a7c-8290-d5cddd51bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for your datasets\n",
    "train_annotation_file = 'wildfire-week8/train/_annotations.coco.json'\n",
    "train_images_dir = 'wildfire-week8/train/original'\n",
    "train_masks_dir = 'wildfire-week8/train/mask'\n",
    "\n",
    "valid_annotation_file = 'wildfire-week8/valid/_annotations.coco.json'\n",
    "valid_images_dir = 'wildfire-week8/valid/original'\n",
    "valid_masks_dir = 'wildfire-week8/valid/mask'\n",
    "\n",
    "test_annotation_file = 'wildfire-week8/test/_annotations.coco.json'\n",
    "test_images_dir = 'wildfire-week8/test/original'\n",
    "test_masks_dir = 'wildfire-week8/test/mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8e270-07ac-4edb-803a-629732f0a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(train_annotation_file, train_images_dir, train_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe450df4-556f-43b9-b5d7-bd5487d66363",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(valid_annotation_file, valid_images_dir, valid_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded84a8-c1b3-413b-a6ed-b214f305a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(test_annotation_file, test_images_dir, test_masks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a569464-cb44-4653-bf9f-07b5c2549304",
   "metadata": {},
   "source": [
    "## Image Segmentation - Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6a941-266b-4f97-b35c-5eec65f3ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model\n",
    "unet_model_water = Unet(\n",
    "    encoder_name=\"efficientnet-b3\",  # Choose your backbone (e.g., \"resnet34\", \"efficientnet-b3\", etc.)\n",
    "    encoder_weights=\"imagenet\",  # Use pre-trained weights\n",
    "    in_channels=3,  # Input channels (e.g., 3 for RGB images)\n",
    "    classes=1  # Number of output classes\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea48b4-96a1-4b15-905e-480beb74e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(unet_model_water )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f87b6-2fa4-4f3e-893e-163c106573d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a plain U-Net model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = self.conv_block(3, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec4 = self.conv_block(1024 + 512, 512)\n",
    "        self.dec3 = self.conv_block(512 + 256, 256)\n",
    "        self.dec2 = self.conv_block(256 + 128, 128)\n",
    "        self.dec1 = self.conv_block(128 + 64, 64)\n",
    "        \n",
    "        # Final layer\n",
    "        self.final = nn.Conv2d(64, 1, kernel_size=1)\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),  # Add padding\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),  # Add padding\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(nn.MaxPool2d(2)(enc1))\n",
    "        enc3 = self.enc3(nn.MaxPool2d(2)(enc2))\n",
    "        enc4 = self.enc4(nn.MaxPool2d(2)(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(nn.MaxPool2d(2)(enc4))\n",
    "        \n",
    "        # Decoding\n",
    "        dec4 = self.dec4(torch.cat((nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)(bottleneck), enc4), dim=1))\n",
    "        dec3 = self.dec3(torch.cat((nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)(dec4), enc3), dim=1))\n",
    "        dec2 = self.dec2(torch.cat((nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)(dec3), enc2), dim=1))\n",
    "        dec1 = self.dec1(torch.cat((nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)(dec2), enc1), dim=1))\n",
    "        \n",
    "        # Final layer\n",
    "        return self.final(dec1)\n",
    "\n",
    "# Initialize the model\n",
    "plain_unet = UNet()\n",
    "\n",
    "# Test the model\n",
    "input_tensor = torch.randn(1, 3, 640, 640)  # Batch size of 1, 3 channels (RGB), 640x640 image\n",
    "output = plain_unet(input_tensor)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c35fa-14ec-4326-9622-16f3b519e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "input_tensor = torch.randn(1, 3, 640, 640)  # Batch size of 1, 3 channels (RGB), 640x640 image\n",
    "output = plain_unet(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b95159-2343-427f-9d33-d5f9ae3f8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = os.listdir(images_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.image_filenames[idx].replace('.jpg', '')+'_mask.png')  # Adjust file extension if needed\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Debugging prints\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image not found or unable to load: {image_path}\")\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Mask not found or unable to load: {mask_path}\")\n",
    "\n",
    "        # Ensure proper dimensions\n",
    "        if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "            raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "\n",
    "        # Resize or pad image and mask to be divisible by 32\n",
    "        # image = self.resize_or_pad(image)\n",
    "        # mask = self.resize_or_pad(mask)\n",
    "\n",
    "        # Normalize image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # mask = mask // 51  # Adjust if necessary\n",
    "        mask = np.where(mask == 4, 1, 0).astype('float32')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32).permute(0, 1, 2), torch.tensor(mask, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3464e-6c94-42f1-b73f-e7ec9b2cb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def count_classes_in_coco(coco_json_file, image_dir):\n",
    "    \"\"\"\n",
    "    Counts the occurrences of each class (category) in the COCO dataset annotations, including the background.\n",
    "    \n",
    "    Args:\n",
    "        coco_json_file (str): Path to the COCO format JSON annotation file.\n",
    "        image_dir (str): Path to the directory containing the original images (to calculate background pixels).\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with class names as keys and their respective counts (in pixels) as values.\n",
    "    \"\"\"\n",
    "    # Load the COCO JSON annotation file\n",
    "    with open(coco_json_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create a dictionary to store class counts\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    # Category mapping (id -> category name)\n",
    "    category_mapping = {category['id']: category['name'] for category in coco_data['categories']}\n",
    "    \n",
    "    # To track pixels covered by objects\n",
    "    covered_pixels_by_image = defaultdict(int)\n",
    "\n",
    "    # Loop through all annotations\n",
    "    for annotation in coco_data['annotations']:\n",
    "        # Get the class id from the annotation (category_id)\n",
    "        class_id = annotation['category_id']\n",
    "        \n",
    "        # Get the image id\n",
    "        image_id = annotation['image_id']\n",
    "        \n",
    "        # Get the area of the segmented mask (to count the number of pixels)\n",
    "        class_counts[class_id] += annotation['area']  # 'area' represents the number of pixels for the object\n",
    "        \n",
    "        # Accumulate the number of pixels covered by objects in each image\n",
    "        covered_pixels_by_image[image_id] += annotation['area']\n",
    "    \n",
    "    # Include background calculation by analyzing total pixels in the images\n",
    "    for image_info in coco_data['images']:\n",
    "        image_id = image_info['id']\n",
    "        image_path = os.path.join(image_dir, image_info['file_name'])\n",
    "        \n",
    "        # Open the image and calculate the total number of pixels\n",
    "        image = Image.open(image_path)\n",
    "        total_pixels = image.width * image.height\n",
    "        \n",
    "        # Calculate background pixels\n",
    "        background_pixels = total_pixels - covered_pixels_by_image[image_id]\n",
    "        \n",
    "        # Add background class count\n",
    "        class_counts[0] += background_pixels  # Assuming class 0 represents background\n",
    "\n",
    "        \n",
    "        keys = list(class_counts.keys())\n",
    "        keys.sort()\n",
    "        sorted_dict = {i: class_counts[i] for i in keys}\n",
    "    return sorted_dict\n",
    "\n",
    "# Example usage:\n",
    "coco_json_path = \"wildfire-week8/train/_annotations.coco.json\"\n",
    "image_directory = \"wildfire-week8/train/original\"\n",
    "class_counts = count_classes_in_coco(coco_json_path, image_directory)\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe2114-f008-4e25-ab17-056578300f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_background = class_counts[0] + class_counts[1] + class_counts[2] + class_counts[3]\n",
    "total_foreground = class_counts[4]\n",
    "pos_weight = total_background / total_foreground\n",
    "\n",
    "print(pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dffd1a-f9cf-4491-bc12-b4c36023797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset_water = SegmentationDataset(train_images_dir, train_masks_dir, transform)\n",
    "valid_dataset_water = SegmentationDataset(valid_images_dir, valid_masks_dir, transform)\n",
    "test_dataset_water = SegmentationDataset(test_images_dir, test_masks_dir, transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_water = DataLoader(train_dataset_water, batch_size=4, shuffle=True)\n",
    "valid_loader_water = DataLoader(valid_dataset_water, batch_size=4, shuffle=False)\n",
    "test_loader_water = DataLoader(test_dataset_water, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d34488-bab6-406f-b3ec-3fecd52680d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Initialize the criterion with pos_weight\n",
    "criterion = nn.BCEWithLogitsLoss(torch.tensor(pos_weight, dtype=torch.float32))\n",
    "optimizer = optim.Adam(plain_unet.parameters(), lr=1e-4)\n",
    "\n",
    "def dice_coeff(pred, target, threshold=0.5):\n",
    "    smooth = 1e-6\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, best_model_path, num_epochs=10, patience=3):\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.unsqueeze(1).float())  # Add channel dimension for masks\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Training loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks.unsqueeze(1).float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(valid_loader)\n",
    "        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Model saved with validation loss: {best_loss:.4f}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bcd1f1-b365-4715-b0bd-d17b2972778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_unet_water_model_path = 'best_unet_water_model_0108.pth'\n",
    "train_model(plain_unet, train_loader_water, valid_loader_water, criterion, optimizer, best_unet_water_model_path, num_epochs=10, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16132484-552b-48eb-af8c-1e9afc0f4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Function to calculate true positives, true negatives, false positives, and false negatives\n",
    "def calculate_tp_fp(pred, mask):\n",
    "    pred_flat = pred.flatten()\n",
    "    mask_flat = mask.flatten()\n",
    "    # True positives are where both pred and mask are 1\n",
    "    true_positives = np.sum((pred_flat == 1) & (mask_flat == 1))\n",
    "    true_negatives = np.sum((pred_flat == 0) & (mask_flat == 0))\n",
    "    # False positives are where pred is 1 but mask is 0\n",
    "    false_positives = np.sum((pred_flat == 1) & (mask_flat == 0))\n",
    "    false_negatives = np.sum((pred_flat == 0) & (mask_flat == 1))\n",
    "    return true_positives, true_negatives, false_positives, false_negatives\n",
    "# Calculate Dice, IoU, and F-score\n",
    "def calculate_dice(tp, fp, fn):\n",
    "    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "\n",
    "def calculate_iou(tp, fp, fn):\n",
    "    return tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "\n",
    "def calculate_fscore(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "def evaluate_model(model, test_loader, save_dir='wildfire-week8/test/testloader-predict', original_images_dir='wildfire-week8/test/testloader-original'):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "    \n",
    "    # Create directories to save predicted masks and original images\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(original_images_dir, exist_ok=True)\n",
    "    \n",
    "    # Collect all images and masks from the test loader\n",
    "    for images, masks in test_loader:\n",
    "        all_images.append(images)\n",
    "        all_masks.append(masks)\n",
    "    all_images = torch.cat(all_images, dim=0)\n",
    "    all_masks = torch.cat(all_masks, dim=0)\n",
    "\n",
    "    tp_total = tn_total = fp_total = fn_total = 0\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(all_images)):\n",
    "            image = all_images[idx].unsqueeze(0).to(torch.float32)  # Get the image\n",
    "            mask = all_masks[idx]  # Get the corresponding mask\n",
    "            output = model(image)  # Get prediction from the model\n",
    "            prediction = (output > 0.5).float().squeeze()  # Binarize the output (threshold = 0.5)\n",
    "            \n",
    "            # Prepare image, mask, and prediction for saving\n",
    "            img_np = image.squeeze().permute(1, 2, 0).cpu().numpy()  # (C, H, W) -> (H, W, C)\n",
    "            mask_np = mask.cpu().numpy()  # Convert mask to numpy\n",
    "            pred_np = prediction.cpu().numpy()  # Convert prediction to numpy\n",
    "            \n",
    "            # Calculate precision, recall, dice, iou, and fscore for the current prediction\n",
    "            tp, tn, fp, fn = calculate_tp_fp(pred_np, mask_np)\n",
    "            tp_total += tp\n",
    "            tn_total += tn\n",
    "            fp_total += fp\n",
    "            fn_total += fn\n",
    "            \n",
    "            # Save the predicted mask as an image\n",
    "            pred_image = torch.tensor(pred_np).unsqueeze(0)  # Add channel dimension\n",
    "            save_image(pred_image, os.path.join(save_dir, f'predicted_mask_{idx}.png'), normalize=True)\n",
    "\n",
    "            # Save the original image\n",
    "            save_image(image.squeeze(0), os.path.join(original_images_dir, f'original_image_{idx}.png'), normalize=True)\n",
    "\n",
    "    # Calculate metrics\n",
    "    pixel_accuracy = (tp_total + tn_total) / (tp_total + tn_total + fp_total + fn_total)\n",
    "    precision = tp_total / (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
    "    recall = tp_total / (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
    "    dice = calculate_dice(tp_total, fp_total, fn_total)\n",
    "    iou = calculate_iou(tp_total, fp_total, fn_total)\n",
    "    fscore = calculate_fscore(precision, recall)\n",
    "    \n",
    "    print(f\"Pixel Accuracy: {pixel_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Dice: {dice:.4f}, IoU: {iou:.4f}, F-score: {fscore:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb478313-c227-48b9-b8af-98d23ead0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of calling this function\n",
    "evaluate_model(plain_unet, test_loader_water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e62f0-6c14-422e-98a9-7d3d7a19243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_with_overlays(model, test_loader, output_folder):\n",
    "    \"\"\"\n",
    "    Displays images with overlayed ground truth and predicted masks, saving only the predicted overlay images.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained PyTorch model used for predictions.\n",
    "    - test_loader: DataLoader providing test images and masks.\n",
    "    - output_folder: Folder where overlayed images will be saved.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure output folder exists\n",
    "    \n",
    "    water_images = []\n",
    "    water_masks = []\n",
    "    \n",
    "    # Collect images with water (mask has value 1)\n",
    "    for images, masks in test_loader:\n",
    "        for i in range(images.size(0)):  # Iterate over batch\n",
    "            # if torch.sum(masks[i] == 1) > 0:  # If the mask contains water (label 1)\n",
    "            water_images.append(images[i].unsqueeze(0))\n",
    "            water_masks.append(masks[i].unsqueeze(0))\n",
    "    \n",
    "    if len(water_images) == 0:\n",
    "        print(\"No water images found in the test set.\")\n",
    "        return\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    water_images = torch.cat(water_images, dim=0)\n",
    "    water_masks = torch.cat(water_masks, dim=0)\n",
    "    \n",
    "    # Define the new mask color as RGB normalized values\n",
    "    new_color = np.array([15/255, 94/255, 156/255])  # Normalize RGB values for 0f5e9c\n",
    "    alpha = 0.9  # Transparency factor\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(water_images)):\n",
    "            image = water_images[idx].to(torch.float32)  # Get the image\n",
    "            mask = water_masks[idx].squeeze(0)  # Get the corresponding mask\n",
    "            \n",
    "            output = model(image.unsqueeze(0))  # Get prediction from the model\n",
    "            prediction = (output > 0.5).float().squeeze(0)  # Binarize the output (threshold = 0.5)\n",
    "            \n",
    "            # Prepare image, mask, and prediction for overlay\n",
    "            img_np = image.squeeze().permute(1, 2, 0).cpu().numpy()  # (C, H, W) -> (H, W, C)\n",
    "            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # Normalize to [0, 1]\n",
    "            \n",
    "            mask_np = mask.cpu().numpy()  # Convert mask to numpy\n",
    "            pred_np = prediction.cpu().numpy().squeeze()  # Convert prediction to numpy and remove extra dimensions\n",
    "            \n",
    "            # Overlay mask onto original image (use custom color)\n",
    "            img_with_mask = img_np.copy()\n",
    "            img_with_mask[mask_np == 1] = new_color  # Apply custom color for ground truth mask\n",
    "            img_with_mask = (alpha * img_with_mask + (1 - alpha) * img_np)  # Blend with original image\n",
    "            \n",
    "            img_with_pred = img_np.copy()\n",
    "            img_with_pred[pred_np == 1] = new_color  # Apply the predicted mask with the custom color\n",
    "            img_with_pred = (alpha * img_with_pred + (1 - alpha) * img_np)  # Blend with original image\n",
    "\n",
    "            # Save only the predicted overlay image\n",
    "            img_filename = f\"predicted_overlay_{idx}.png\"\n",
    "            img_path = os.path.join(output_folder, img_filename)\n",
    "\n",
    "            # Plot all three images\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            # Display original image\n",
    "            axs[0].imshow(img_np)\n",
    "            axs[0].set_title('Original Image')\n",
    "            axs[0].axis('off')\n",
    "            \n",
    "            # Display original image with ground truth mask\n",
    "            axs[1].imshow(img_with_mask)\n",
    "            axs[1].set_title('Original Image with Ground Truth Mask')\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            # Display original image with predicted mask\n",
    "            axs[2].imshow(img_with_pred)\n",
    "            axs[2].set_title('Original Image with Predicted Mask')\n",
    "            axs[2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()  # Show the three images\n",
    "            \n",
    "            # Save the predicted overlay image only\n",
    "            plt.imsave(img_path, img_with_pred)  # Save the predicted overlay image\n",
    "            plt.close(fig)  # Close the figure to avoid display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cbdd4-f270-428f-b062-23d9ff5b3665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_predictions_with_overlays(unet_model_water, test_loader_water, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a454e0c-ce32-4fe6-ad73-d22b74502d30",
   "metadata": {},
   "source": [
    "## Apply Alive Tree Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e34214-7113-49ab-93c6-b1381eca76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def color_based_segmentation(image):\n",
    "    \"\"\"Generates a mask for alive trees based on color segmentation.\"\"\"\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_green = np.array([25, 40, 20])   # Lowered brightness and saturation to capture darker greens\n",
    "    upper_green = np.array([100, 255, 180])\n",
    "    green_mask = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "\n",
    "    return green_mask\n",
    "\n",
    "def process_images(original_folder, water_mask_folder, water_mask_applied_folder, output_format='png'):\n",
    "    \"\"\"Processes all images in the specified folders,\n",
    "       applies tree detection methods, and visualizes results.\n",
    "       \n",
    "    Parameters:\n",
    "    - output_format: The format to save the output images ('png', 'jpg', 'tiff', etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all original images\n",
    "    original_images = [f for f in os.listdir(original_folder) if f.lower().endswith(('jpg', 'png', 'jpeg'))]\n",
    "    \n",
    "    # Loop through each original image file\n",
    "    for image_file in original_images:\n",
    "        # Read the original image\n",
    "        original_image_path = os.path.join(original_folder, image_file)\n",
    "        original_image = cv2.imread(original_image_path)\n",
    "        if original_image is None:\n",
    "            print(f\"Error: Could not read original image from {original_image_path}.\")\n",
    "            continue\n",
    "\n",
    "        # Read the corresponding water mask (e.g., predicted_mask_0.png)\n",
    "        water_mask_file = f'predicted_mask_{image_file.split(\".\")[0].split(\"_\")[-1]}.png'\n",
    "        water_mask_path = os.path.join(water_mask_folder, water_mask_file)\n",
    "        water_mask = cv2.imread(water_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if water_mask is None:\n",
    "            print(f\"Error: Could not read water mask from {water_mask_path}.\")\n",
    "            continue\n",
    "\n",
    "        # Read the water mask applied image (e.g., predicted_overlay_1.png)\n",
    "        water_mask_applied_file = f'predicted_overlay_{image_file.split(\".\")[0].split(\"_\")[-1]}.png'\n",
    "        water_mask_applied_path = os.path.join(water_mask_applied_folder, water_mask_applied_file)\n",
    "        water_mask_applied_image = cv2.imread(water_mask_applied_path)\n",
    "\n",
    "        if water_mask_applied_image is None:\n",
    "            print(f\"Error: Could not read water mask applied image from {water_mask_applied_path}.\")\n",
    "            continue\n",
    "\n",
    "        # Generate the mask for alive trees\n",
    "        alive_tree_mask = color_based_segmentation(original_image)\n",
    "\n",
    "        # Exclude areas identified as water in the water mask\n",
    "        alive_tree_mask[water_mask > 0] = 0  # Set alive tree mask to 0 where water mask is present\n",
    "\n",
    "        # Create transparent overlay for alive trees\n",
    "        alive_tree_overlay = np.zeros_like(original_image, dtype=np.uint8)\n",
    "        alive_tree_overlay[alive_tree_mask > 0] = [0, 255, 0]  # Green for alive trees\n",
    "\n",
    "        # Combine overlays with the provided water mask applied image\n",
    "        alpha_trees = 0.8  # Transparency for tree overlay\n",
    "        final_overlay_image = cv2.addWeighted(water_mask_applied_image, 1, alive_tree_overlay, alpha_trees, 0)\n",
    "\n",
    "        # Save the final overlay image\n",
    "        output_filename = f'final_overlay_{os.path.splitext(image_file)[0]}.{output_format}'\n",
    "        output_path = os.path.join(water_mask_applied_folder, output_filename)  # Save in the same folder\n",
    "        cv2.imwrite(output_path, final_overlay_image)\n",
    "\n",
    "        # Visualize the original image, water mask applied image, and final image with tree overlay\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Show original image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Show water mask applied image\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(cv2.cvtColor(water_mask_applied_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Image with Water Mask Applied')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Show final image with both overlays\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(cv2.cvtColor(final_overlay_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Final Image with Alive Tree Overlay')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Final overlay images saved in '{water_mask_applied_folder}' with format '{output_format}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f0bcf-0a73-4254-bcd3-ba1b00429857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "original_folder = 'wildfire-week8/test/testloader-original'        # Change this to your image folder path\n",
    "water_mask_folder = 'wildfire-week8/test/testloader-predict'\n",
    "water_mask_applied_folder = 'wildfire-week8/test/water-overlay'# Change this to your water mask folder path\n",
    "# output_folder = 'wildfire-week8/test/hsv'  \n",
    "process_images(original_folder, water_mask_folder, water_mask_applied_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804130f3-98e3-47d2-86aa-15b2a286a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def color_based_segmentation(image):\n",
    "#     \"\"\"Generates green and combined (green + yellow) masks for alive trees based on color segmentation.\"\"\"\n",
    "#     hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "#     # Green color range\n",
    "#     lower_green = np.array([35, 50, 40])\n",
    "#     upper_green = np.array([100, 255, 200])\n",
    "#     green_mask = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "    \n",
    "#     # # Refined bright yellow color range\n",
    "#     # lower_yellow = np.array([15, 120, 180])  # Targeting a brighter yellow with high saturation\n",
    "#     # upper_yellow = np.array([30, 200, 255])   # Allowing for a broader range without high brightness \n",
    "#     # yellow_mask = cv2.inRange(hsv_image, lower_yellow, upper_yellow)\n",
    "#     lower_green2 = np.array([25, 40, 20])   # Lowered brightness and saturation to capture darker greens\n",
    "#     upper_green2 = np.array([100, 255, 180])\n",
    "#     green_mask2 = cv2.inRange(hsv_image, lower_green2, upper_green2)\n",
    "#     # Combined mask (green + yellow)\n",
    "#     # combined_mask = cv2.bitwise_or(green_mask, yellow_mask)\n",
    "    \n",
    "#     return green_mask, green_mask2\n",
    "\n",
    "# def process_and_visualize_images(image_folder):\n",
    "#     \"\"\"Processes each image, generates green and combined masks, and visualizes the results.\"\"\"\n",
    "#     # Get all image files\n",
    "#     image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
    "    \n",
    "#     for image_file in image_files:\n",
    "#         # Read the original image\n",
    "#         image_path = os.path.join(image_folder, image_file)\n",
    "#         image = cv2.imread(image_path)\n",
    "        \n",
    "#         if image is None:\n",
    "#             print(f\"Error: Could not read image from {image_path}.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Generate green and combined masks\n",
    "#         green_mask, combined_mask = color_based_segmentation(image)\n",
    "        \n",
    "#         # Create color overlays for the masks\n",
    "#         green_overlay = np.zeros_like(image, dtype=np.uint8)\n",
    "#         green_overlay[green_mask > 0] = [0, 255, 0]  # Green for green mask\n",
    "        \n",
    "#         combined_overlay = np.zeros_like(image, dtype=np.uint8)\n",
    "#         combined_overlay[combined_mask > 0] = [0, 255, 0]  # Green for the combined mask (both green and yellow)\n",
    "        \n",
    "#         # Apply the overlays to the original image\n",
    "#         alpha = 0.9  # Transparency level\n",
    "#         green_applied = cv2.addWeighted(image, 1, green_overlay, alpha, 0)\n",
    "#         combined_applied = cv2.addWeighted(image, 1, combined_overlay, alpha, 0)\n",
    "        \n",
    "#         # Visualization\n",
    "#         plt.figure(figsize=(15, 5))\n",
    "        \n",
    "#         # Original image\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "#         plt.title('Original Image')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         # Green mask applied on original image\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(cv2.cvtColor(green_applied, cv2.COLOR_BGR2RGB))\n",
    "#         plt.title('Green Mask Applied')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         # Combined mask applied on original image\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(cv2.cvtColor(combined_applied, cv2.COLOR_BGR2RGB))\n",
    "#         plt.title('Combined Mask Applied (Green + Yellow)')\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "# process_and_visualize_images(original_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b4309-213f-48a1-b937-a2ae5c0fac8b",
   "metadata": {},
   "source": [
    "## Image Segmentation - Three Types of Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12db20-f81e-4792-8ae7-5e03707c7072",
   "metadata": {},
   "source": [
    "### Unet + Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc811f04-4568-47bc-84c6-46f92e742bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "unet_model_tree = Unet(\n",
    "    encoder_name=\"resnet50\",  # Choose your backbone (e.g., \"resnet34\", \"efficientnet-b3\", etc.)\n",
    "    encoder_weights=\"imagenet\",  # Use pre-trained weights\n",
    "    in_channels=3,  # Input channels (e.g., 3 for RGB images)\n",
    "    classes=4  # Number of output classes\n",
    ")\n",
    "\n",
    "# Example input\n",
    "input_tensor = torch.randn(1, 3, 640, 640)  # Batch size of 1, 3 channels (RGB), 640x640 image\n",
    "output = unet_model_tree(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfd31b-a995-4730-a6e4-08ca39d8201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fd46e-2b41-4ef3-afae-6af326fab894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = os.listdir(images_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and corresponding mask\n",
    "        image_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.image_filenames[idx].replace('.jpg', '') + '_mask.png')\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Debugging print statements for troubleshooting\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image not found or unable to load: {image_path}\")\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Mask not found or unable to load: {mask_path}\")\n",
    "\n",
    "        # Ensure the image has 3 channels\n",
    "        if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "            raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "\n",
    "        # Convert BGR (OpenCV format) to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert pixel value 4 to 0 in the mask\n",
    "        mask[mask == 4] = 0\n",
    "\n",
    "        \n",
    "\n",
    "        # Convert image and mask to PyTorch tensors\n",
    "        image_tensor = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # Change to (C, H, W)\n",
    "        mask_tensor = torch.tensor(mask, dtype=torch.long)\n",
    "        # Optionally apply transformations to the image (e.g., augmentations, normalization)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return the image and mask as tensors\n",
    "        return image_tensor, mask_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595e038-8460-4794-8be0-2ee351448c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset_tree = SegmentationDataset(train_images_dir, train_masks_dir, transform)\n",
    "valid_dataset_tree = SegmentationDataset(valid_images_dir, valid_masks_dir, transform)\n",
    "test_dataset_tree = SegmentationDataset(test_images_dir, test_masks_dir, transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_tree = DataLoader(train_dataset_tree, batch_size=4, shuffle=True)\n",
    "valid_loader_tree = DataLoader(valid_dataset_tree, batch_size=4, shuffle=False)\n",
    "test_loader_tree = DataLoader(test_dataset_tree, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8199bd8-53e0-4252-be88-5d9a8234e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_classes_in_dataloader(dataloader):\n",
    "    \"\"\"\n",
    "    Counts the occurrences of each class in the dataset based on the masks provided by the DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): The PyTorch DataLoader containing images and masks.\n",
    "        \n",
    "    Returns:\n",
    "        class_counts (dict): A dictionary where keys are class labels and values are their respective counts.\n",
    "    \"\"\"\n",
    "    class_counts = defaultdict(int)  # Dictionary to hold class counts\n",
    "\n",
    "    # Iterate over the DataLoader batches\n",
    "    for images, masks in dataloader:\n",
    "        # Flatten the mask and count occurrences of each class label\n",
    "        unique_labels, counts = torch.unique(masks, return_counts=True)\n",
    "\n",
    "        # Update the class_counts dictionary with counts from the current batch\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            class_counts[int(label.item())] += int(count.item())\n",
    "\n",
    "    return dict(class_counts)  # Convert defaultdict to a regular dict before returning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cad8b4-04c3-465c-9d85-199aa50477d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = count_classes_in_dataloader(train_loader_tree)\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961a26f-e4aa-4ee2-868e-c0a184f10bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_class_weights(class_counts):\n",
    "    \"\"\"\n",
    "    Calculate class weights based on the class counts from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        class_counts (dict): A dictionary with class IDs or names as keys and pixel counts as values.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of class weights to be used in nn.CrossEntropyLoss.\n",
    "    \"\"\"\n",
    "    # Get the total number of pixels across all classes\n",
    "    total_pixels = sum(class_counts.values())\n",
    "\n",
    "    # Calculate the frequency of each class (number of pixels of class / total pixels)\n",
    "    class_frequencies = {cls: count / total_pixels for cls, count in class_counts.items()}\n",
    "\n",
    "    # Invert the frequencies to give higher weights to less frequent classes\n",
    "    class_weights = {cls: 1.0 / freq if freq > 0 else 0.0 for cls, freq in class_frequencies.items()}\n",
    "\n",
    "    # Normalize weights so that they sum to 1 or use them directly\n",
    "    weight_values = list(class_weights.values())\n",
    "    weights_tensor = torch.tensor(weight_values, dtype=torch.float32)\n",
    "\n",
    "    return weights_tensor\n",
    "\n",
    "\n",
    "\n",
    "# Calculate weights based on class counts\n",
    "class_weights = calculate_class_weights(class_counts)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ababf-b105-4e4f-9bfd-f468ef7536c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# You can now use these weights in nn.CrossEntropyLoss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(unet_model_tree.parameters(), lr=1e-4)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "best_unet_model_tree_path = \"wildfire-week8/best_unet_model_tree.pth\"  # Path to save the best model\n",
    "\n",
    "# Training loop with early stopping, saving, and restoring the best model\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, best_model_path, num_epochs=10, patience=3):\n",
    "    best_valid_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())  # Copy initial model weights\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        print(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement in validation loss\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            early_stopping_counter = 0  # Reset counter if validation loss improves\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())  # Store best weights\n",
    "            torch.save(best_model_weights, best_model_path)  # Save best model to disk\n",
    "            print(f\"Validation loss improved to {valid_loss:.4f}. Best model saved to {best_model_path}.\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"No improvement in validation loss for {early_stopping_counter} epoch(s).\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Restore the best model weights after early stopping\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print(\"Best model weights restored from saved model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90de614-68ce-46db-9a40-0a08e50986bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with early stopping, saving, and restoring best weights\n",
    "train_model(unet_model_tree, train_loader_tree, valid_loader_tree, criterion, optimizer, best_unet_model_tree_path, num_epochs=10, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ceca7-27e7-4780-a911-642e0d2db68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate_model(model, data_loader, criterion, class_names):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_pixels = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Initialize counters for each class\n",
    "    num_classes = len(class_names)\n",
    "    class_correct = torch.zeros(num_classes)  # Correct predictions per class\n",
    "    class_total = torch.zeros(num_classes)    # Total ground truth pixels per class\n",
    "    class_predicted = torch.zeros(num_classes)  # Total predicted pixels per class\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in data_loader:\n",
    "            preds = model(images)\n",
    "\n",
    "            # Ensure labels are of shape [batch_size, height, width]\n",
    "            if labels.dim() == 4:  # If labels have an extra dimension\n",
    "                labels = labels.squeeze(1)  # Remove the channel dimension\n",
    "\n",
    "            # Convert labels to Long type (int64)\n",
    "            labels = labels.long()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class for each pixel\n",
    "            _, predicted = torch.max(preds, 1)  # Shape: [batch_size, height, width]\n",
    "\n",
    "            # Update overall accuracy (correct_predictions / total_pixels)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_pixels += labels.numel()  # Total number of pixels in the batch\n",
    "\n",
    "            # Update class-wise correct and total counts\n",
    "            for class_index in range(num_classes):\n",
    "                # Correct pixels for each class\n",
    "                class_correct[class_index] += ((predicted == class_index) & (labels == class_index)).sum().item()\n",
    "                # Total ground truth pixels for each class\n",
    "                class_total[class_index] += (labels == class_index).sum().item()\n",
    "                # Total predicted pixels for each class\n",
    "                class_predicted[class_index] += (predicted == class_index).sum().item()\n",
    "\n",
    "    # Calculate average loss and overall pixel accuracy\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    pixel_accuracy = correct_predictions / total_pixels\n",
    "\n",
    "    # Calculate pixel accuracy for each class, avoiding division by zero\n",
    "    class_pixel_accuracy = class_correct / class_total\n",
    "    class_pixel_accuracy[class_total == 0] = 0  # Set accuracy to 0 for classes with no pixels\n",
    "\n",
    "    # Calculate precision and recall for each class\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "\n",
    "    for class_index in range(num_classes):\n",
    "        if class_total[class_index] > 0:  # Avoid division by zero\n",
    "            # Precision: True Positives / (True Positives + False Positives)\n",
    "            precision[class_index] = class_correct[class_index] / class_predicted[class_index] if class_predicted[class_index] > 0 else 0\n",
    "            # Recall: True Positives / (True Positives + False Negatives)\n",
    "            recall[class_index] = class_correct[class_index] / class_total[class_index]\n",
    "\n",
    "    # Create a DataFrame to display results, excluding the first class\n",
    "    results_df = pd.DataFrame({\n",
    "        'Class Name': class_names[1:],  # Exclude the first class (background)\n",
    "        'Pixel Accuracy': class_pixel_accuracy[1:].numpy(),  # Exclude the first class\n",
    "        'Precision': precision[1:],  # Exclude the first class\n",
    "        'Recall': recall[1:],  # Exclude the first class\n",
    "    })\n",
    "\n",
    "    # Round to three decimal places\n",
    "    results_df[['Pixel Accuracy', 'Precision', 'Recall']] = results_df[['Pixel Accuracy', 'Precision', 'Recall']].round(3)\n",
    "\n",
    "    return average_loss, pixel_accuracy, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e607b-b0fe-41fc-b066-6b3a88b288c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Background', 'Beetle Trees', 'Dead Tree', 'Debris']  # Adjust class names as needed\n",
    "average_loss, pixel_accuracy, results_table = evaluate_model(unet_model_tree, test_loader_tree,torch.nn.CrossEntropyLoss() , class_names)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bdb05c-a046-4bac-989e-a70369480aba",
   "metadata": {},
   "source": [
    "### Unet + Moblenet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d389e-5abd-4630-8327-f3c0953cac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_mobilenet_model = Unet(\n",
    "    encoder_name=\"mobilenet_v2\",  # Choose your backbone (e.g., \"resnet34\", \"efficientnet-b3\", etc.)\n",
    "    encoder_weights=\"imagenet\",  # Use pre-trained weights\n",
    "    in_channels=3,  # Input channels (e.g., 3 for RGB images)\n",
    "    classes=4  # Number of output classes\n",
    ")\n",
    "# Example input\n",
    "input_tensor = torch.randn(1, 3, 640, 640)  # Batch size of 1, 3 channels (RGB), 640x640 image\n",
    "output = unet_model_tree(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6497c-43a5-4b82-a487-005e08d1c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now use these weights in nn.CrossEntropyLoss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(unet_mobilenet_model.parameters(), lr=1e-4)\n",
    "best_unet_mobilenet_model_path = \"wildfire-week7/best_unet_mobilenet_model_tree.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4c2b7-eb43-452d-9ec7-be0fd8597a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with early stopping, saving, and restoring best weights\n",
    "train_model(unet_mobilenet_model, train_loader_tree, valid_loader_tree, criterion, optimizer, best_unet_mobilenet_model_path, num_epochs=10, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d4e5f-3d1d-4e26-9d07-8d8913b6dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss, pixel_accuracy, results_table = evaluate_model(unet_mobilenet_model, test_loader_tree,torch.nn.CrossEntropyLoss() , class_names)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe7807-fd4f-4cea-9483-7895d275732d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
