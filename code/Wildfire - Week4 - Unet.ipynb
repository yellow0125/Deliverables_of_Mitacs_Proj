{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93137-8f05-44d5-b378-a0b46296832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as maskUtils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800cca9-b765-432e-8bd8-ac94d457668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Paths\n",
    "# annotation_file = 'wildfire-week3-v3/test/_annotations.coco.json'  # Replace with your annotations file\n",
    "# image_dir = 'wildfire-week3-v3/test/original'                       # Replace with your images directory\n",
    "# mask_dir = 'wildfire-week3-v3/test/mask'            # Directory to save masks\n",
    "def generate_mask(annotation_file, image_dir, mask_dir): \n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "    # Initialize COCO API\n",
    "    coco = COCO(annotation_file)\n",
    "\n",
    "# Get all image IDs\n",
    "    image_ids = coco.getImgIds()\n",
    "\n",
    "# Define category mapping\n",
    "    categories = coco.loadCats(coco.getCatIds())\n",
    "    category_mapping = {cat['id']: cat['name'] for cat in categories}\n",
    "    num_classes = len(category_mapping)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    for image_id in tqdm(image_ids, desc=\"Generating Multi-Class Masks\"):\n",
    "    # Load image info\n",
    "        image_info = coco.loadImgs(image_id)[0]\n",
    "        height, width = image_info['height'], image_info['width']\n",
    "\n",
    "    # Initialize blank multi-class mask\n",
    "        multi_class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Get all annotations for the image\n",
    "        ann_ids = coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = coco.loadAnns(ann_ids)\n",
    "\n",
    "        for ann in annotations:\n",
    "            category_id = ann['category_id']\n",
    "            ann_id = ann['id']\n",
    "\n",
    "        # Generate mask for the annotation\n",
    "            if ann['iscrowd']:\n",
    "                rle = ann['segmentation']\n",
    "                mask = maskUtils.decode(rle)\n",
    "            else:\n",
    "                mask = coco.annToMask(ann)\n",
    "\n",
    "        # Assign category_id to mask pixels\n",
    "        # Handle overlapping by assigning the category_id (latest object overwrites)\n",
    "            multi_class_mask[mask > 0] = category_id\n",
    "\n",
    "    # Save the multi-class mask as PNG\n",
    "        image_filename = image_info['file_name']\n",
    "        mask_filename = os.path.splitext(image_filename)[0] + '_mask.png'\n",
    "        mask_path = os.path.join(mask_dir, mask_filename)\n",
    "        cv2.imwrite(mask_path, multi_class_mask)\n",
    "\n",
    "    \n",
    "# Create mask directory if it doesn't exist\n",
    "\n",
    "\n",
    "    # Optional: Visualize the mask overlay\n",
    "    '''\n",
    "    # Load original image\n",
    "    image_path = os.path.join(image_dir, image_filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create color map\n",
    "    unique_categories = np.unique(multi_class_mask)\n",
    "    unique_categories = unique_categories[unique_categories != 0]  # Exclude background\n",
    "    num_unique = len(unique_categories)\n",
    "    colors = plt.cm.get_cmap('hsv', num_unique + 1)\n",
    "\n",
    "    color_map = {}\n",
    "    for idx, category_id in enumerate(unique_categories, start=1):\n",
    "        color_map[category_id] = (np.array(colors(idx)[:3]) * 255).astype(np.uint8)\n",
    "\n",
    "    # Create color mask\n",
    "    color_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    for category_id, color in color_map.items():\n",
    "        color_mask[multi_class_mask == category_id] = color\n",
    "\n",
    "    # Overlay mask on image\n",
    "    overlay = cv2.addWeighted(image_rgb, 0.7, color_mask, 0.3, 0)\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Image ID: {image_id} with Mask Overlay\")\n",
    "    plt.show()\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16749555-09f3-4430-966f-ab425d193df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for your datasets\n",
    "train_annotation_file = 'wildfire-week4/train/_annotations.coco.json'\n",
    "train_images_dir = 'wildfire-week4/train/original'\n",
    "train_masks_dir = 'wildfire-week4/train/mask'\n",
    "\n",
    "valid_annotation_file = 'wildfire-week4/valid/_annotations.coco.json'\n",
    "valid_images_dir = 'wildfire-week4/valid/original'\n",
    "valid_masks_dir = 'wildfire-week4/valid/mask'\n",
    "\n",
    "test_annotation_file = 'wildfire-week4/test/_annotations.coco.json'\n",
    "test_images_dir = 'wildfire-week4/test/original'\n",
    "test_masks_dir = 'wildfire-week4/test/mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a03acb-89b7-4dc3-9309-10f825a9a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(train_annotation_file, train_images_dir, train_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe85fd-50b6-4d2f-a279-5a301a21398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(valid_annotation_file, valid_images_dir, valid_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d2ab0-a54a-41cd-8164-4a365f1921f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(test_annotation_file, test_images_dir, test_masks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f0b83-39a0-418e-9c35-caa5b66b3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "# Initialize the model\n",
    "unet_model = Unet(\n",
    "    encoder_name=\"resnet50\",  # Choose your backbone (e.g., \"resnet34\", \"efficientnet-b3\", etc.)\n",
    "    encoder_weights=\"imagenet\",  # Use pre-trained weights\n",
    "    in_channels=3,  # Input channels (e.g., 3 for RGB images)\n",
    "    classes=5  # Number of output classes\n",
    ")\n",
    "\n",
    "# Example input\n",
    "input_tensor = torch.randn(1, 3, 640, 640)  # Batch size of 1, 3 channels (RGB), 640x640 image\n",
    "output = unet_model(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68bf031-9a8d-4a33-9fbf-f83760bb33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = os.listdir(images_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.image_filenames[idx].replace('.jpg', '')+'_mask.png')  # Adjust file extension if needed\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Debugging prints\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image not found or unable to load: {image_path}\")\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Mask not found or unable to load: {mask_path}\")\n",
    "\n",
    "        # Ensure proper dimensions\n",
    "        if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "            raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "\n",
    "        # Resize or pad image and mask to be divisible by 32\n",
    "        # image = self.resize_or_pad(image)\n",
    "        # mask = self.resize_or_pad(mask)\n",
    "\n",
    "        # Normalize image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # mask = mask // 51  # Adjust if necessary\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32).permute(0, 1, 2), torch.tensor(mask, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d655e41-5423-4d6f-987f-56b7fff9f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# # Directories for your datasets\n",
    "# train_images_dir = 'wildfire-week3-v3/train/original'\n",
    "# train_masks_dir = 'wildfire-week3-v3/train/mask'\n",
    "# valid_images_dir = 'wildfire-week3-v3/valid/original'\n",
    "# valid_masks_dir = 'wildfire-week3-v3/valid/mask'\n",
    "# test_images_dir = 'wildfire-week3-v3/test/original'\n",
    "# test_masks_dir = 'wildfire-week3-v3/test/mask'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform)\n",
    "valid_dataset = SegmentationDataset(valid_images_dir, valid_masks_dir, transform)\n",
    "test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c059f9a-6cce-49de-bc9d-07e3703ca8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "best_model_path = \"wildfire-week4/best_model.pth\"  # Path to save the best model\n",
    "\n",
    "# Training loop with early stopping, saving, and restoring the best model\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10, patience=3):\n",
    "    best_valid_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())  # Copy initial model weights\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        print(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement in validation loss\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            early_stopping_counter = 0  # Reset counter if validation loss improves\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())  # Store best weights\n",
    "            torch.save(best_model_weights, best_model_path)  # Save best model to disk\n",
    "            print(f\"Validation loss improved to {valid_loss:.4f}. Best model saved to {best_model_path}.\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"No improvement in validation loss for {early_stopping_counter} epoch(s).\")\n",
    "\n",
    "        # Early stopping condition\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Restore the best model weights after early stopping\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    print(\"Best model weights restored from saved model.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081f166-d042-47f4-90a7-8c48e13b62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with early stopping, saving, and restoring best weights\n",
    "train_model(unet_model, train_loader, valid_loader, torch.nn.CrossEntropyLoss(), optimizer, num_epochs=10, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61787d2-644f-4486-a71f-2148a0fd8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model_classname(model, data_loader, criterion, class_names):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_pixels = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Initialize counters for each class\n",
    "    num_classes = len(class_names)\n",
    "    class_correct = torch.zeros(num_classes)  # Correct predictions per class\n",
    "    class_total = torch.zeros(num_classes)    # Total ground truth pixels per class\n",
    "    class_predicted = torch.zeros(num_classes)  # Total predicted pixels per class\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in data_loader:\n",
    "            preds = model(images)\n",
    "\n",
    "            # Ensure labels are of shape [batch_size, height, width]\n",
    "            if labels.dim() == 4:  # If labels have an extra dimension\n",
    "                labels = labels.squeeze(1)  # Remove the channel dimension\n",
    "\n",
    "            # Convert labels to Long type (int64)\n",
    "            labels = labels.long()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class for each pixel\n",
    "            _, predicted = torch.max(preds, 1)  # Shape: [batch_size, height, width]\n",
    "\n",
    "            # Update overall accuracy (correct_predictions / total_pixels)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_pixels += labels.numel()  # Total number of pixels in the batch\n",
    "\n",
    "            # Update class-wise correct and total counts\n",
    "            for class_index in range(num_classes):\n",
    "                # Correct pixels for each class\n",
    "                class_correct[class_index] += ((predicted == class_index) & (labels == class_index)).sum().item()\n",
    "                # Total ground truth pixels for each class\n",
    "                class_total[class_index] += (labels == class_index).sum().item()\n",
    "                # Total predicted pixels for each class\n",
    "                class_predicted[class_index] += (predicted == class_index).sum().item()\n",
    "\n",
    "    # Calculate average loss and overall pixel accuracy\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    pixel_accuracy = correct_predictions / total_pixels\n",
    "\n",
    "    # Calculate pixel accuracy for each class, avoiding division by zero\n",
    "    class_pixel_accuracy = class_correct / class_total\n",
    "    class_pixel_accuracy[class_total == 0] = 0  # Set accuracy to 0 for classes with no pixels\n",
    "\n",
    "    # Calculate precision and recall for each class\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "\n",
    "    for class_index in range(num_classes):\n",
    "        if class_total[class_index] > 0:  # Avoid division by zero\n",
    "            # Precision: True Positives / (True Positives + False Positives)\n",
    "            precision[class_index] = class_correct[class_index] / class_predicted[class_index] if class_predicted[class_index] > 0 else 0\n",
    "            # Recall: True Positives / (True Positives + False Negatives)\n",
    "            recall[class_index] = class_correct[class_index] / class_total[class_index]\n",
    "\n",
    "    # Create a DataFrame to display results, excluding the first class\n",
    "    results_df = pd.DataFrame({\n",
    "        'Class Name': class_names[1:],  # Exclude the first class (background)\n",
    "        'Pixel Accuracy': class_pixel_accuracy[1:].numpy(),  # Exclude the first class\n",
    "        'Precision': precision[1:],  # Exclude the first class\n",
    "        'Recall': recall[1:],  # Exclude the first class\n",
    "    })\n",
    "\n",
    "    # Round to three decimal places\n",
    "    results_df[['Pixel Accuracy', 'Precision', 'Recall']] = results_df[['Pixel Accuracy', 'Precision', 'Recall']].round(3)\n",
    "\n",
    "    return average_loss, pixel_accuracy, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28815cfa-9c21-49a0-8e78-a81f9d80fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, class_names):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_pixels = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Initialize counters for each class\n",
    "    num_classes = len(class_names)\n",
    "    class_correct = torch.zeros(num_classes)  # Correct predictions per class\n",
    "    class_total = torch.zeros(num_classes)    # Total ground truth pixels per class\n",
    "    class_predicted = torch.zeros(num_classes)  # Total predicted pixels per class\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in data_loader:\n",
    "            preds = model(images)\n",
    "\n",
    "            # Ensure labels are of shape [batch_size, height, width]\n",
    "            if labels.dim() == 4:  # If labels have an extra dimension\n",
    "                labels = labels.squeeze(1)  # Remove the channel dimension\n",
    "\n",
    "            # Convert labels to Long type (int64)\n",
    "            labels = labels.long()\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class for each pixel\n",
    "            _, predicted = torch.max(preds, 1)  # Shape: [batch_size, height, width]\n",
    "\n",
    "            # Update overall accuracy (correct_predictions / total_pixels)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_pixels += labels.numel()  # Total number of pixels in the batch\n",
    "\n",
    "            # Update class-wise correct and total counts\n",
    "            for class_index in range(num_classes):\n",
    "                # Correct pixels for each class\n",
    "                class_correct[class_index] += ((predicted == class_index) & (labels == class_index)).sum().item()\n",
    "                # Total ground truth pixels for each class\n",
    "                class_total[class_index] += (labels == class_index).sum().item()\n",
    "                # Total predicted pixels for each class\n",
    "                class_predicted[class_index] += (predicted == class_index).sum().item()\n",
    "\n",
    "    # Calculate average loss and overall pixel accuracy\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    pixel_accuracy = correct_predictions / total_pixels\n",
    "\n",
    "    # Calculate pixel accuracy for each class, avoiding division by zero\n",
    "    class_pixel_accuracy = class_correct / class_total\n",
    "    class_pixel_accuracy[class_total == 0] = 0  # Set accuracy to 0 for classes with no pixels\n",
    "\n",
    "    # Calculate precision and recall for each class\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "\n",
    "    for class_index in range(num_classes):\n",
    "        if class_total[class_index] > 0:  # Avoid division by zero\n",
    "            # Precision: True Positives / (True Positives + False Positives)\n",
    "            precision[class_index] = class_correct[class_index] / class_predicted[class_index] if class_predicted[class_index] > 0 else 0\n",
    "            # Recall: True Positives / (True Positives + False Negatives)\n",
    "            recall[class_index] = class_correct[class_index] / class_total[class_index]\n",
    "\n",
    "    # Create a DataFrame to display results, excluding the first class\n",
    "    results_df = pd.DataFrame({\n",
    "        'Class Name': class_names[1:],  # Exclude the first class (background)\n",
    "        'Pixel Accuracy': class_pixel_accuracy[1:].numpy(),  # Exclude the first class\n",
    "        'Precision': precision[1:],  # Exclude the first class\n",
    "        'Recall': recall[1:],  # Exclude the first class\n",
    "    })\n",
    "\n",
    "    # Round to three decimal places\n",
    "    results_df[['Pixel Accuracy', 'Precision', 'Recall']] = results_df[['Pixel Accuracy', 'Precision', 'Recall']].round(3)\n",
    "\n",
    "    return average_loss, pixel_accuracy, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79c238-eab1-4f34-aeae-fb7568bbf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5  # Adjust this according to your number of classes\n",
    "average_loss, pixel_accuracy, class_pixel_accuracy, precision, recall, f1_score = evaluate_model(unet_model, test_loader, torch.nn.CrossEntropyLoss(), num_classes)\n",
    "\n",
    "print(f'Validation Loss: {average_loss:.4f}')\n",
    "print(f'Pixel Accuracy (Overall): {pixel_accuracy * 100:.4f}%')\n",
    "for i in range(num_classes):\n",
    "    print(f'Pixel Accuracy for Class {i}: {class_pixel_accuracy[i] * 100:.4f}%')\n",
    "    print(f'Precision for Class {i}: {precision[i] * 100:.4f}%')\n",
    "    print(f'Recall for Class {i}: {recall[i] * 100:.4f}%')\n",
    "    print(f'F1 Score for Class {i}: {f1_score[i] * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1319a8-8c05-4873-b266-c806027094fb",
   "metadata": {},
   "source": [
    "It has better accuracy for class 1, 2 and 3 and when epoch = 1 than ephch  = 10. So we will apply a weigth to fix imbalance class category problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb2d77-ab90-4bd2-a7c5-9f193c1eccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def count_classes_in_coco(coco_json_file, image_dir):\n",
    "    \"\"\"\n",
    "    Counts the occurrences of each class (category) in the COCO dataset annotations, including the background.\n",
    "    \n",
    "    Args:\n",
    "        coco_json_file (str): Path to the COCO format JSON annotation file.\n",
    "        image_dir (str): Path to the directory containing the original images (to calculate background pixels).\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with class names as keys and their respective counts (in pixels) as values.\n",
    "    \"\"\"\n",
    "    # Load the COCO JSON annotation file\n",
    "    with open(coco_json_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create a dictionary to store class counts\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    # Category mapping (id -> category name)\n",
    "    category_mapping = {category['id']: category['name'] for category in coco_data['categories']}\n",
    "    \n",
    "    # To track pixels covered by objects\n",
    "    covered_pixels_by_image = defaultdict(int)\n",
    "\n",
    "    # Loop through all annotations\n",
    "    for annotation in coco_data['annotations']:\n",
    "        # Get the class id from the annotation (category_id)\n",
    "        class_id = annotation['category_id']\n",
    "        \n",
    "        # Get the image id\n",
    "        image_id = annotation['image_id']\n",
    "        \n",
    "        # Get the area of the segmented mask (to count the number of pixels)\n",
    "        class_counts[class_id] += annotation['area']  # 'area' represents the number of pixels for the object\n",
    "        \n",
    "        # Accumulate the number of pixels covered by objects in each image\n",
    "        covered_pixels_by_image[image_id] += annotation['area']\n",
    "    \n",
    "    # Include background calculation by analyzing total pixels in the images\n",
    "    for image_info in coco_data['images']:\n",
    "        image_id = image_info['id']\n",
    "        image_path = os.path.join(image_dir, image_info['file_name'])\n",
    "        \n",
    "        # Open the image and calculate the total number of pixels\n",
    "        image = Image.open(image_path)\n",
    "        total_pixels = image.width * image.height\n",
    "        \n",
    "        # Calculate background pixels\n",
    "        background_pixels = total_pixels - covered_pixels_by_image[image_id]\n",
    "        \n",
    "        # Add background class count\n",
    "        class_counts[0] += background_pixels  # Assuming class 0 represents background\n",
    "\n",
    "        \n",
    "        keys = list(class_counts.keys())\n",
    "        keys.sort()\n",
    "        sorted_dict = {i: class_counts[i] for i in keys}\n",
    "    return sorted_dict\n",
    "\n",
    "# Example usage:\n",
    "coco_json_path = \"wildfire-week4/train/_annotations.coco.json\"\n",
    "image_directory = \"wildfire-week4/train/original\"\n",
    "class_counts = count_classes_in_coco(coco_json_path, image_directory)\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8aa4a2-487d-4bda-9b6c-f921832902af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_class_weights(class_counts):\n",
    "    \"\"\"\n",
    "    Calculate class weights based on the class counts from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        class_counts (dict): A dictionary with class IDs or names as keys and pixel counts as values.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of class weights to be used in nn.CrossEntropyLoss.\n",
    "    \"\"\"\n",
    "    # Get the total number of pixels across all classes\n",
    "    total_pixels = sum(class_counts.values())\n",
    "\n",
    "    # Calculate the frequency of each class (number of pixels of class / total pixels)\n",
    "    class_frequencies = {cls: count / total_pixels for cls, count in class_counts.items()}\n",
    "\n",
    "    # Invert the frequencies to give higher weights to less frequent classes\n",
    "    class_weights = {cls: 1.0 / freq if freq > 0 else 0.0 for cls, freq in class_frequencies.items()}\n",
    "\n",
    "    # Normalize weights so that they sum to 1 or use them directly\n",
    "    weight_values = list(class_weights.values())\n",
    "    weights_tensor = torch.tensor(weight_values, dtype=torch.float32)\n",
    "\n",
    "    return weights_tensor\n",
    "\n",
    "\n",
    "\n",
    "# Calculate weights based on class counts\n",
    "class_weights = calculate_class_weights(class_counts)\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# You can now use these weights in nn.CrossEntropyLoss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b65d3-5783-4867-bb88-581188a4eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(unet_model, train_loader, valid_loader, criterion, optimizer, num_epochs=10, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439db950-6533-4f7c-9fda-4aaf77143f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5  # Adjust this according to your number of classes\n",
    "average_loss, pixel_accuracy, class_pixel_accuracy, precision, recall, f1_score = evaluate_model(unet_model, test_loader, torch.nn.CrossEntropyLoss(), num_classes)\n",
    "\n",
    "print(f'Validation Loss: {average_loss:.4f}')\n",
    "print(f'Pixel Accuracy (Overall): {pixel_accuracy * 100:.4f}%')\n",
    "for i in range(num_classes):\n",
    "    print(f'Pixel Accuracy for Class {i}: {class_pixel_accuracy[i] * 100:.4f}%')\n",
    "    print(f'Precision for Class {i}: {precision[i] * 100:.4f}%')\n",
    "    print(f'Recall for Class {i}: {recall[i] * 100:.4f}%')\n",
    "    print(f'F1 Score for Class {i}: {f1_score[i] * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93a4d9-76f2-4b6e-9c87-98a054241760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Background', 'Beetle Trees', 'Dead Tree', 'Debris', 'Water']  # Adjust class names as needed\n",
    "average_loss, pixel_accuracy, results_table = evaluate_model_classname(unet_model, test_loader,torch.nn.CrossEntropyLoss() , class_names)\n",
    "print(results_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590581e-f586-4aa5-9849-2989b3feb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Function to apply color mask over an image\n",
    "def apply_mask(image, mask, alpha=0.3):\n",
    "    \"\"\"\n",
    "    Apply a color mask over the image.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Original image (H, W, 3).\n",
    "        mask (numpy.ndarray): Mask to overlay (H, W).\n",
    "        alpha (float): Transparency level for overlay.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Image with mask overlay.\n",
    "    \"\"\"\n",
    "    # Create a color map for visualization, assuming 4 classes + background\n",
    "    color_map = np.array([\n",
    "        [0, 0, 0],        # background\n",
    "        [255, 0, 0],      # class 1 (beetles tree) - Red\n",
    "        [0, 255, 0],      # class 2 (dead tree) - Green\n",
    "        [0, 0, 255],      # class 3 (debris) - Blue\n",
    "        [255, 255, 0]     # class 4 (water) - Yellow\n",
    "    ])\n",
    "\n",
    "    colored_mask = color_map[mask]\n",
    "    overlay = (image * (1 - alpha) + colored_mask * alpha).astype(np.uint8)\n",
    "    return overlay\n",
    "\n",
    "def visualize_predictions_with_ytrue(test_loader, model, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualizes random predictions from the test loader with ground truth masks and \n",
    "    overlays both ground truth and predicted masks on the original image.\n",
    "    \n",
    "    Args:\n",
    "        test_loader (DataLoader): DataLoader for test dataset.\n",
    "        model (torch.nn.Module): Trained model for prediction.\n",
    "        num_samples (int): Number of random samples to visualize.\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples: Each tuple contains (original_image, y_true_overlay, pred_overlay)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    output_list = []\n",
    "\n",
    "    # Select a random batch from test_loader\n",
    "    images, true_masks = next(iter(test_loader))\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    # Convert predictions to class labels\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    \n",
    "    # Move tensors back to numpy for visualization\n",
    "    images = images.numpy().transpose(0, 2, 3, 1)  # Convert to (N, H, W, C)\n",
    "    preds = preds.numpy()                          # Predicted masks\n",
    "    true_masks = true_masks.numpy()                # Ground truth masks\n",
    "\n",
    "    # Randomly select indices to visualize\n",
    "    selected_indices = random.sample(range(images.shape[0]), num_samples)\n",
    "    \n",
    "    # Iterate over selected indices\n",
    "    for idx in selected_indices:\n",
    "        image = (images[idx] * 255).astype(np.uint8)  # Rescale image back to 0-255 range\n",
    "        true_mask = true_masks[idx]\n",
    "        pred_mask = preds[idx]\n",
    "\n",
    "        # Apply ground truth mask and predicted mask over original image\n",
    "        y_true_overlay = apply_mask(image, true_mask)\n",
    "        pred_overlay = apply_mask(image, pred_mask)\n",
    "\n",
    "        # Store the original image and overlays in a tuple\n",
    "        output_list.append((image, y_true_overlay, pred_overlay))\n",
    "\n",
    "        # Plot for visualization\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[0].imshow(image)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(y_true_overlay)\n",
    "        axs[1].set_title(\"Y_true Overlay\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        axs[2].imshow(pred_overlay)\n",
    "        axs[2].set_title(\"Predicted Overlay\")\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return output_list\n",
    "\n",
    "# Example usage:\n",
    "output_list = visualize_predictions_with_ytrue(test_loader, unet_model, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c864e-2b09-4c9b-b83b-cfcf52ca1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def watershed_segmentation(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Remove noise\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "    # Sure background area\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "    # Sure foreground area\n",
    "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "\n",
    "    # Unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "    # Marker labelling\n",
    "    ret, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "    # Add one to all labels so that sure background is not 0, but 1\n",
    "    markers = markers + 1\n",
    "\n",
    "    # Mark the unknown region with 0\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    # Apply watershed\n",
    "    markers = cv2.watershed(image, markers)\n",
    "    image[markers == -1] = [255, 0, 0]\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe265050-d1bb-48fb-9346-5af2767aac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "def detect_trees_using_watershed(image_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Loop through each image in the folder\n",
    "    for image_file in os.listdir(image_folder):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is None:\n",
    "            print(f\"Unable to load image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply a threshold to create a binary image\n",
    "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "        # Noise removal using morphological operations\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "        # Sure background area (dilation)\n",
    "        sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "        # Finding sure foreground area using distance transform\n",
    "        dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "        _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "\n",
    "        # Finding unknown region\n",
    "        sure_fg = np.uint8(sure_fg)\n",
    "        unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "        # Marker labelling\n",
    "        _, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "        # Add 1 to all markers to distinguish background from the unknown region\n",
    "        markers = markers + 1\n",
    "\n",
    "        # Mark the unknown region as zero\n",
    "        markers[unknown == 255] = 0\n",
    "\n",
    "        # Apply the watershed algorithm\n",
    "        markers = cv2.watershed(image, markers)\n",
    "\n",
    "        # Boundaries detected by watershed will be marked as -1\n",
    "        image[markers == -1] = [0, 0, 255]\n",
    "\n",
    "        # Save the output\n",
    "        output_image_path = os.path.join(output_folder, image_file)\n",
    "        cv2.imwrite(output_image_path, image)\n",
    "\n",
    "        # Optionally display the result\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.title('Watershed Segmentation Result')\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_folder = \"wildfire-week3-v3/train/original\"\n",
    "output_folder = \"wildfire-week3-v3/train/alive\"\n",
    "detect_trees_using_watershed(image_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9323e38-4352-4e07-9389-e224fd0720c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def detect_alive_trees_in_images(image_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Loop through each image in the folder\n",
    "    for image_file in os.listdir(image_folder):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"Unable to load image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Convert the image to HSV for color segmentation\n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Define HSV color range for green (live trees)\n",
    "        lower_green = np.array([35, 40, 40])\n",
    "        upper_green = np.array([85, 255, 255])\n",
    "\n",
    "        # Create a mask for green areas (live trees)\n",
    "        mask = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "\n",
    "        # Apply contour detection on the mask\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Draw contours on the original image to show detected tree crowns\n",
    "        result_image = image.copy()\n",
    "        cv2.drawContours(result_image, contours, -1, (0, 255, 0), 2)  # Green color for contours\n",
    "\n",
    "        # Save the mask and the result image\n",
    "        mask_output_path = os.path.join(output_folder, image_file.replace('.jpg', '_mask.jpg'))\n",
    "        result_output_path = os.path.join(output_folder, image_file.replace('.jpg', '_result.jpg'))\n",
    "\n",
    "        cv2.imwrite(mask_output_path, mask)\n",
    "        cv2.imwrite(result_output_path, result_image)\n",
    "\n",
    "        # Optionally, display the result\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('Original Image with Detected Tree Crowns')\n",
    "        plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title('Mask of Alive Trees')\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "output_folder_contour = 'wildfire-week3-v3/train/alive_contour'\n",
    "detect_alive_trees_in_images(image_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f2f84-6c2e-41f9-b00a-87447edcc032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_texture_features(gray_image):\n",
    "    # Compute GLCM for various angles and distances\n",
    "    distances = [1]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    glcm = graycomatrix(gray_image, distances, angles, symmetric=True, normed=True)\n",
    "\n",
    "    # Calculate texture features from GLCM\n",
    "    contrast = graycoprops(glcm, 'contrast')[0]\n",
    "    dissimilarity = graycoprops(glcm, 'dissimilarity')[0]\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity')[0]\n",
    "    energy = graycoprops(glcm, 'energy')[0]\n",
    "    correlation = graycoprops(glcm, 'correlation')[0]\n",
    "    entropy = -np.sum(glcm * np.log(glcm + np.finfo(float).eps))\n",
    "\n",
    "    return contrast, dissimilarity, homogeneity, energy, correlation, entropy\n",
    "\n",
    "def detect_trees_with_texture(image_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for image_filename in os.listdir(image_folder):\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error loading image: {image_filename}\")\n",
    "            continue\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Gaussian Blur\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        # Calculate texture features\n",
    "        contrast, dissimilarity, homogeneity, energy, correlation, entropy = calculate_texture_features(blurred)\n",
    "\n",
    "        # Example thresholding based on one of the texture features (adjust as needed)\n",
    "        if contrast.all() > 0.6:  # This threshold value is arbitrary and should be tuned\n",
    "            mask = cv2.threshold(blurred, 128, 255, cv2.THRESH_BINARY)[1]\n",
    "        else:\n",
    "            mask = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        # Morphological operations to clean up the mask\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        cleaned_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Apply the mask to the original image\n",
    "        masked_image = cv2.bitwise_and(image, image, mask=cleaned_mask)\n",
    "\n",
    "        # Save the output mask\n",
    "        output_mask_path = os.path.join(output_folder, f\"tree_mask_{image_filename}\")\n",
    "        cv2.imwrite(output_mask_path, cleaned_mask)\n",
    "\n",
    "        # Visualize the original image and the masked image\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Original Image: {image_filename}\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Masked Image\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "output_folder = 'wildfire-week3-v3/train/all_trees'  # Change to your output folder path\n",
    "detect_trees_with_texture(image_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b6df7-7818-4eac-89fb-64bdf6713bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def texture_analysis(image_gray):\n",
    "    \"\"\"Perform texture analysis using GLCM and return a texture mask.\"\"\"\n",
    "    glcm = graycomatrix(image_gray, distances=[5], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')\n",
    "    texture_mask = np.zeros_like(image_gray, dtype=np.uint8)\n",
    "    \n",
    "    # Threshold on texture (adjust based on the specific image and texture properties)\n",
    "    texture_mask[contrast[0, 0] > 0.1] = 255\n",
    "    \n",
    "    return texture_mask\n",
    "\n",
    "def color_segmentation(image):\n",
    "    \"\"\"Perform color-based segmentation and return a color mask.\"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_green = np.array([35, 40, 40])\n",
    "    upper_green = np.array([85, 255, 255])\n",
    "    color_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    return color_mask\n",
    "\n",
    "def apply_transparent_mask(image, mask, alpha=0.4):\n",
    "    \"\"\"Apply a transparent mask to the original image.\"\"\"\n",
    "    overlay = image.copy()\n",
    "    \n",
    "    # Colorize the mask (green in this case, but you can adjust the color)\n",
    "    mask_colored = np.zeros_like(image)\n",
    "    mask_colored[:, :, 1] = mask  # Apply mask to the green channel\n",
    "    \n",
    "    # Apply the transparent overlay (alpha blending)\n",
    "    cv2.addWeighted(mask_colored, alpha, overlay, 1 - alpha, 0, overlay)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "def detect_trees(image):\n",
    "    \"\"\"Detect trees in the image using color and texture masks.\"\"\"\n",
    "    color_mask = color_segmentation(image)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    texture_mask = texture_analysis(gray_image)\n",
    "    combined_mask = cv2.bitwise_and(color_mask, texture_mask)\n",
    "    \n",
    "    # Morphological operations to clean the mask\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    cleaned_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return cleaned_mask\n",
    "\n",
    "def process_image_folder(folder_path, output_folder):\n",
    "    \"\"\"Process all images in a folder, apply transparent masks, and save the results.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, file_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            if image is not None:\n",
    "                # Detect trees and get the mask\n",
    "                tree_mask = detect_trees(image)\n",
    "                \n",
    "                # Apply transparency mask to the original image\n",
    "                result_image = apply_transparent_mask(image, tree_mask)\n",
    "                \n",
    "                # Save the result\n",
    "                result_path = os.path.join(output_folder, f\"masked_{file_name}\")\n",
    "                cv2.imwrite(result_path, result_image)\n",
    "                print(f\"Processed and saved: {result_path}\")\n",
    "                \n",
    "                # Optionally display the result\n",
    "                display_image(image, result_image)\n",
    "            else:\n",
    "                print(f\"Error reading image: {image_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_path = 'wildfire-week4/test/original'\n",
    "output_folder = 'wildfire-week4/test/all_trees'\n",
    "\n",
    "process_image_folder(folder_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80614c-8c49-422c-bc2d-ed875954a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(original, masked):\n",
    "    \"\"\"Display the original and masked image using Matplotlib.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Masked image with transparency\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(masked, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Masked Image with Transparency')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3257ab-3964-4623-a81b-4c3a98c18635",
   "metadata": {},
   "source": [
    "## Train dataset with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f26aed-437f-4e92-87d2-a4713b1e62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for your datasets\n",
    "train_annotation_file_da = 'wildfire-week4-v2/train/_annotations.coco.json'\n",
    "train_images_dir_da = 'wildfire-week4-v2/train/original'\n",
    "train_masks_dir_da = 'wildfire-week4-v2/train/mask'\n",
    "\n",
    "valid_annotation_file_da = 'wildfire-week4-v2/valid/_annotations.coco.json'\n",
    "valid_images_dir_da = 'wildfire-week4-v2/valid/original'\n",
    "valid_masks_dir_da = 'wildfire-week4-v2/valid/mask'\n",
    "\n",
    "test_annotation_file_da = 'wildfire-week4/test/_annotations.coco.json'\n",
    "test_images_dir_da = 'wildfire-week4-v2/test/original'\n",
    "test_masks_dir_da = 'wildfire-week4-v2/test/mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7190f1dd-1ee9-41b1-8980-1e2a3dbeea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(train_annotation_file_da, train_images_dir_da, train_masks_dir_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82607f-a3d1-4c42-8be7-2c21ec99ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(valid_annotation_file_da, valid_images_dir_da, valid_masks_dir_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a7892-643f-448e-8a81-dbac68ffafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mask(test_annotation_file_da, test_images_dir_da, test_masks_dir_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac7cc7-61bd-42dc-87b1-7395ecf1d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model_da = Unet(\n",
    "    encoder_name=\"resnet50\",  # Choose your backbone (e.g., \"resnet34\", \"efficientnet-b3\", etc.)\n",
    "    encoder_weights=\"imagenet\",  # Use pre-trained weights\n",
    "    in_channels=3,  # Input channels (e.g., 3 for RGB images)\n",
    "    classes=5  # Number of output classes\n",
    ")\n",
    "input_tensor = torch.randn(1, 3, 640, 640)  # Batch size of 1, 3 channels (RGB), 640x640 image\n",
    "output = unet_model_da(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c367a15-d11d-4a18-9ff6-53200e3e5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset_da = SegmentationDataset(train_images_dir_da, train_masks_dir_da, transform)\n",
    "valid_dataset_da = SegmentationDataset(valid_images_dir_da, valid_masks_dir_da, transform)\n",
    "test_dataset_da = SegmentationDataset(test_images_dir_da, test_masks_dir_da, transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_da = DataLoader(train_dataset_da, batch_size=4, shuffle=True)\n",
    "valid_loader_da = DataLoader(valid_dataset_da, batch_size=4, shuffle=False)\n",
    "test_loader_da = DataLoader(test_dataset_da, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df917da7-a1d0-4240-a1cd-c41f7c7bec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_json_path_da = \"wildfire-week4-v2/train/_annotations.coco.json\"\n",
    "image_directory_da = \"wildfire-week4-v2/train/original\"\n",
    "class_counts = count_classes_in_coco(coco_json_path_da, image_directory_da)\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac80a9-4ad6-4968-a519-8ccf0ae6a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_da = calculate_class_weights(class_counts)\n",
    "print(\"Class Weights:\", class_weights_da)\n",
    "\n",
    "# You can now use these weights in nn.CrossEntropyLoss\n",
    "criterion_da = torch.nn.CrossEntropyLoss(weight=class_weights_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b4609-2ffa-4c65-a211-ec3bb1ef112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(unet_model_da.parameters(), lr=1e-4)\n",
    "train_model(unet_model_da, train_loader_da, valid_loader_da, criterion_da, optimizer, num_epochs=10, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c8d73-7765-48a7-aae8-6cb72a23d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss, pixel_accuracy, results_table = evaluate_model_classname(unet_model_da, test_loader_da,torch.nn.CrossEntropyLoss() , class_names)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00186e2-a27a-49ce-9edc-a20b77d8cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = visualize_predictions_with_ytrue(test_loader_da, unet_model, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992a1be-9ebe-40bd-9677-ab752e871d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_exg(image):\n",
    "    # Convert the image from BGR to RGB\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Split the image into R, G, and B channels\n",
    "    R = rgb_image[:, :, 0].astype(float)\n",
    "    G = rgb_image[:, :, 1].astype(float)\n",
    "    B = rgb_image[:, :, 2].astype(float)\n",
    "    \n",
    "    # Calculate Excess Green Index (ExG)\n",
    "    ExG = (2 * G - R - B) / (2 * G + R + B + 1e-10)  # Adding a small constant to avoid division by zero\n",
    "\n",
    "    return ExG\n",
    "\n",
    "def apply_mask(original_image, exg_image, alpha=0.5):\n",
    "    # Normalize ExG for masking\n",
    "    exg_image_normalized = cv2.normalize(exg_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    exg_image_normalized = exg_image_normalized.astype(np.uint8)\n",
    "\n",
    "    # Calculate quantile-based threshold for trees\n",
    "    threshold_value_tree = np.quantile(exg_image_normalized, 0.6)  # 75th percentile for trees\n",
    "\n",
    "    # Create a binary mask for trees\n",
    "    tree_mask = np.zeros_like(exg_image_normalized)\n",
    "    tree_mask[exg_image_normalized > threshold_value_tree] = 255  # Trees\n",
    "\n",
    "    # Create RGBA image for visualization\n",
    "    rgba_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGBA)\n",
    "\n",
    "    # Create a color image for the tree mask\n",
    "    tree_color = np.zeros_like(rgba_image)\n",
    "    tree_color[tree_mask == 255] = [0, 255, 0, 255]  # Green for trees\n",
    "\n",
    "    # Convert the tree mask to have an alpha channel\n",
    "    tree_color = cv2.cvtColor(tree_color, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "    # Blend the original image with the tree mask using transparency\n",
    "    blended_image = cv2.addWeighted(rgba_image, 1 - alpha, tree_color, alpha, 0)\n",
    "\n",
    "    return tree_mask, blended_image\n",
    "\n",
    "\n",
    "def visualize_images(original_image, mask, masked_image):\n",
    "    # Create a figure to visualize the original and masked images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('ExG Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Masked Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def process_images_in_folder(input_folder, output_folder):\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for image files\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            # Read the image\n",
    "            original_image = cv2.imread(file_path)\n",
    "            if original_image is None:\n",
    "                print(f\"Could not read image: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # Calculate ExG\n",
    "            exg_image = calculate_exg(original_image)\n",
    "\n",
    "            # Apply mask to the original image\n",
    "            mask, masked_image = apply_mask(original_image, exg_image)\n",
    "\n",
    "            # Save the ExG mask\n",
    "            output_file_path_mask = os.path.join(output_folder, f\"exg_mask_{filename}\")\n",
    "            cv2.imwrite(output_file_path_mask, mask)\n",
    "            print(f\"Saved ExG mask: {output_file_path_mask}\")\n",
    "\n",
    "            # Visualize the images\n",
    "            visualize_images(original_image, mask, masked_image)\n",
    "\n",
    "\n",
    "\n",
    "# Set the input and output folder paths\n",
    "input_folder = 'wildfire-week4/train/original'  # Change this to your input folder\n",
    "output_folder = 'wildfire-week4/train/exg'  # Change this to your desired output folder\n",
    "\n",
    "# Process the images in the specified folder\n",
    "process_images_in_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e717b-3402-4851-a2ec-a98f512a1134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
